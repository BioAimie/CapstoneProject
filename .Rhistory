setwd('~/Coursera/CapstoneProject/')
en.blogs <- readLines('en_US/en_US.blogs.txt', n = 1000)
en.blogs[1]
a <- en.blogs[1]
b <- tolower(strsplit(a, '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(a, '')[[1]])))]
c <- strsplit(paste(b, collapse=''), ' ')[[1]]
d <- data.frame(Words = c, Record = 1)
d
hist(with(d, aggregate(Record~Words, FUN=sum))$Record)
a
b
c
d
c
head(d)
c
lapply(2:length(c), function(x) c[(x-1):x])
lapply(2:length(c), function(x) w <- c[(x-1):x])
w
lapply(2:length(c), function(x) c[(x-1):x])
do.call(rbind, lapply(2:length(c), function(x) c[(x-1):x]))
rm(c)
a
b
tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
i <- 1
tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
strsplit(en.blogs[i], '')
tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
for (i in 1:1) {
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
}
rm(a,b,d)
elan.chunk
clean.chunk
sorted_words <- names(sort(table(strsplit(tolower(paste(readLines("http://www.norvig.com/big.txt"), collapse = " ")), "[^a-z]+")), decreasing = TRUE))
readLines("http://www.norvig.com/big.txt")
en.blogs <- readLines('en_US/en_US.blogs.txt', n = 1000)
paste(en.blogs, collapse = ' ')
sorted_words <- names(sort(table(strsplit(tolower(paste(readLines("en_US/en_US.blogs.txt"), collapse = " ")), "[^a-z]+")), decreasing = TRUE))
correct <- function(word) { c(sorted_words[ adist(word, sorted_words) <= min(adist(word, sorted_words), 2)], word)[1] }
head(correct)
correct('piese')
correct('cakke')
nLines <- 1000
en.blogs <- readLines('en_US/en_US.blogs.txt', n = nLines)
sorted_words <- names(sort(table(strsplit(tolower(paste(readLines("en_US/en_US.blogs.txt", n = nLines), collapse = " ")), "[^a-z]+")), decreasing = TRUE))
correct <- function(word) { c(sorted_words[ adist(word, sorted_words) <= min(adist(word, sorted_words), 2)], word)[1] }
head(clean.chunk)
clean.charVec
clean.chunk
correct(clean.chunk)
correct(clean.chunk[1])
correct(clean.chunk[10])
clean.chunk
length(clean.chunk)
sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
# sample the words in en.blogs and correct spelling mistakes using Peter Norvig algorithm
sorted_words <- names(sort(table(strsplit(tolower(paste(en.blogs, collapse = " ")), "[^a-z]+")), decreasing = TRUE))
correct <- function(word) { c(sorted_words[ adist(word, sorted_words) <= min(adist(word, sorted_words), 2)], word)[1] }
head(clean.chunk)
clean.chunk
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
clean.chunk
sapply(1:length(clean.chunk), function(x) length(which(clean.chunk == clean.chunk[x])))
clean.chunk
lapply(2:length(clean.chunk), function(x) clean.chunk[(x-1):x])
do.call(rbind, lapply(2:length(clean.chunk), function(x) clean.chunk[(x-1):x]))
do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' ')))
data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1)
data.frame(Combo = clean.chunk, Count = 1)
with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.1
library(ggplot2)
ggplot(gram.1, aes(x=Combo, y=Count)) + geom_bar(stat='identity')
ggplot(gram.2, aes(x=Combo, y=Count)) + geom_bar(stat='identity')
gram.1
gram.1.all <- c()
gram.2.all <- c()
gram.3.all <- c()
for (i in 1:1) {
# clean the line and reformat into character vector
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
# perform an n-gram analysis
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.3 <- with(data.frame(Combo = do.call(rbind, lapply(3:length(clean.chunk), function(x) paste(clean.chunk[(x-2):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
# bind the results onto the data frames that will hold frequencies for each line
gram.1.all <- rbind(gram.1.all, gram.1)
gram.2.all <- rbind(gram.2.all, gram.2)
gram.3.all <- rbind(gram.3.all, gram.3)
}
i
rm(i)
gram.1.all <- c()
gram.2.all <- c()
gram.3.all <- c()
for (i in 1:1) {
# clean the line and reformat into character vector
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
# perform an n-gram analysis
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.3 <- with(data.frame(Combo = do.call(rbind, lapply(3:length(clean.chunk), function(x) paste(clean.chunk[(x-2):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
# bind the results onto the data frames that will hold frequencies for each line
gram.1.all <- rbind(gram.1.all, gram.1)
gram.2.all <- rbind(gram.2.all, gram.2)
gram.3.all <- rbind(gram.3.all, gram.3)
}
gram.1
clean.chunk
i <- 2
tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
clean.chunk
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.3 <- with(data.frame(Combo = do.call(rbind, lapply(3:length(clean.chunk), function(x) paste(clean.chunk[(x-2):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.1
gram.2
gram.3
gram.1.all <- c()
gram.2.all <- c()
gram.3.all <- c()
for (i in 1:nLines) {
# clean the line and reformat into character vector
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
# perform an n-gram analysis
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.3 <- with(data.frame(Combo = do.call(rbind, lapply(3:length(clean.chunk), function(x) paste(clean.chunk[(x-2):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
# bind the results onto the data frames that will hold frequencies for each line
gram.1.all <- rbind(gram.1.all, gram.1)
gram.2.all <- rbind(gram.2.all, gram.2)
gram.3.all <- rbind(gram.3.all, gram.3)
}
en.blogs[68]
en.blogs[67:69]
readLines('en_US/en_US.blogs.txt', n = 1)
en.blogs[1]
correct('Rm25')
en.blogs[68]
clean.chunk
for (i in 69:nLines) {
# clean the line and reformat into character vector
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
# perform an n-gram analysis
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.1.all <- rbind(gram.1.all, gram.1)
if(length(clean.chunk < 3)) {
next()
} else {
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.3 <- with(data.frame(Combo = do.call(rbind, lapply(3:length(clean.chunk), function(x) paste(clean.chunk[(x-2):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2.all <- rbind(gram.2.all, gram.2)
gram.3.all <- rbind(gram.3.all, gram.3)
}
}
en.blogs[608]
correct(en.blogs[608])
data.frame(sample = 'Hello', count = 1)
with(data.frame(sample = 'Hello', count = 1), aggregate(count~sample, FUN=sum))
for (i in 609:nLines) {
# clean the line and reformat into character vector
clean.charVec <- tolower(strsplit(en.blogs[i], '')[[1]])[grep(paste(' ',paste(letters, collapse='|'),sep='|'), gsub('\\.|\\?|\\!|\\(|\\)','', tolower(strsplit(en.blogs[i], '')[[1]])))]
clean.chunk <- strsplit(paste(clean.charVec, collapse=''), ' ')[[1]]
clean.chunk <- sapply(1:length(clean.chunk), function(x) correct(clean.chunk[x]))
# perform an n-gram analysis... only keep lines where there are at least 3 words
if(length(clean.chunk < 3)) {
next()
} else {
gram.1 <- with(data.frame(Combo = clean.chunk, Count = 1), aggregate(Count~Combo, FUN=sum))
gram.2 <- with(data.frame(Combo = do.call(rbind, lapply(2:length(clean.chunk), function(x) paste(clean.chunk[(x-1):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.3 <- with(data.frame(Combo = do.call(rbind, lapply(3:length(clean.chunk), function(x) paste(clean.chunk[(x-2):x], collapse=' '))), Count = 1), aggregate(Count~Combo, FUN=sum))
gram.1.all <- rbind(gram.1.all, gram.1)
gram.2.all <- rbind(gram.2.all, gram.2)
gram.3.all <- rbind(gram.3.all, gram.3)
}
}
gram.1.agg <- with(gram.1.all, aggregate(Count~Combo, FUN=sum))
gram.2.agg <- with(gram.2.all, aggregate(Count~Combo, FUN=sum))
gram.3.agg <- with(gram.3.all, aggregate(Count~Combo, FUN=sum))
head(gram.1.agg)
gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE))]
gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ]
gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100]
gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100, ]
gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100, ]
ggplot(gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity')
factor(gram.1.agg$Combo, levels = gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), 'Combo'])
gram.1.agg$Combo <- factor(gram.1.agg$Combo, levels = gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), 'Combo'])
gram.2.agg$Combo <- factor(gram.2.agg$Combo, levels = gram.2.agg[with(gram.2.agg, order(Count, decreasing = TRUE)), 'Combo'])
gram.3.agg$Combo <- factor(gram.3.agg$Combo, levels = gram.3.agg[with(gram.3.agg, order(Count, decreasing = TRUE)), 'Combo'])
ggplot(gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity')
ggplot(gram.2.agg[with(gram.2.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity')
ggplot(gram.3.agg[with(gram.3.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity')
ggplot(gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity') + theme(axis.text.x=element_text(angle=90, hjust=1))
ggplot(gram.2.agg[with(gram.2.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity') + theme(axis.text.x=element_text(angle=90, hjust=1))
ggplot(gram.3.agg[with(gram.3.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity') + theme(axis.text.x=element_text(angle=90, hjust=1))
head(gram.1.agg)
hist(gram.1.agg$Count)
hist(gram.1.agg$Count, breaks = 50)
hist(gram.1.agg$Count, breaks = 50, probability = TRUE)
hist(gram.1.agg$Count, breaks = 50, frequency = TRUE)
hist(gram.1.agg$Count, breaks = 50, freq = TRUE)
plot(gram.1.agg$Count, log='y', type='h', lwd=10, lend=2)
plot(gram.2.agg$Count, log='y', type='h', lwd=10, lend=2)
plot(gram.3.agg$Count, log='y', type='h', lwd=10, lend=2)
Sys.Date()
en.blogs <- readLines('en_US/en_US.blogs.txt')
en.news <- readLines('en_US/en_US.news.txt')
en.tweets <- readLines('en_US/en_US.twitter.txt')
en.tweets
length(en.glogs)
length(en.blogs)
en.blogs[1]
en.blogs[1:2]
paste(en.blogs[1:2], collapse = ' ')
length(paste(en.blogs[1:2], collapse = ' '))
strsplit(paste(en.blogs[1:2], collapse = ' '), ' ')
length(strsplit(paste(en.blogs[1:2], collapse = ' '), ' '))
length(strsplit(paste(en.blogs[1:2], collapse = ' '), ' ')[[1]])
object.size(en.blogs)
object.size(en.blogs)/1000
object.size(en.blogs)/1000000
object.size(en.blogs)/1000000
c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000)
data.frame(DataSet = c('Blogs','News','Twitter') , Size = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
data.frame(DataSet = c('Blogs','News','Twitter') , Size(MB) = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
data.frame(DataSet = c('Blogs','News','Twitter') , 'Size (MB)' = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
data.frame(DataSet = c('Blogs','News','Twitter') , 'Size_(MB)' = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
data.frame(DataSet = c('Blogs','News','Twitter') , 'Size_MB' = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
data.frame(DataSet = c('Blogs','News','Twitter') , 'Size_MB' = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
data.frame(DataSet = c('Blogs','News','Twitter') , 'Size_MB' = c(object.size(en.blogs)/1000000, object.size(en.news)/1000000, object.size(en.tweets)/1000000))
length(strsplit(paste(en.blogs[1:2], collapse = ' ')[[1]], ' '))
length(strsplit(paste(en.blogs[1:2], collapse = ' '), ' ')[[1]])
length(strsplit(paste(en.blogs, collapse = ' '), ' ')[[1]])
words.blogs <- length(strsplit(paste(en.blogs, collapse = ' '), ' ')[[1]])
words.news <- length(strsplit(paste(en.news, collapse = ' '), ' ')[[1]])
words.tweets <- length(strsplit(paste(en.tweets, collapse = ' '), ' ')[[1]])
length(en.blogs)
lines.blogs <- length(en.blogs)
lines.news <- length(en.news)
lines.tweets <- length(en.tweets)
size.blogs <- object.size(en.blogs)/1000000
size.news <- object.size(en.news)/1000000
size.tweets <- object.size(en.tweets)/1000000
dataSets <- c('Blogs','News','Twitter')
data.frame(DataSets = dataSets, Size_MB = c(size.blogs, size.news, size.tweets), Lines = c(lines.blogs, lines.news, lines.tweets), Words = c(words.blogs, words.news, words.tweets))
table(data.frame(DataSets = dataSets, Size_MB = c(size.blogs, size.news, size.tweets), Lines = c(lines.blogs, lines.news, lines.tweets), Words = c(words.blogs, words.news, words.tweets)))
print(data.frame(DataSets = dataSets, Size_MB = c(size.blogs, size.news, size.tweets), Lines = c(lines.blogs, lines.news, lines.tweets), Words = c(words.blogs, words.news, words.tweets)))
print(data.frame(DataSets = dataSets, Size_MB = c(size.blogs, size.news, size.tweets), Lines = c(lines.blogs, lines.news, lines.tweets), Words = c(words.blogs, words.news, words.tweets)))
print(data.frame(DataSets = dataSets, Size_MB = c(size.blogs, size.news, size.tweets), Lines = c(lines.blogs, lines.news, lines.tweets), Words = c(words.blogs, words.news, words.tweets)))
summary.stats <- data.frame(DataSets = dataSets, Size_MB = c(size.blogs, size.news, size.tweets), Lines = c(lines.blogs, lines.news, lines.tweets), Words = c(words.blogs, words.news, words.tweets))
summary.stats
View(summary.stats)
read.csv('cheatSheet.csv', header = TRUE, sep = ',')
read.csv('cheatSheet.csv', header = TRUE, sep = ',')
summary.stats
format(summary.stats$Lines)
format(summary.stats$Lines, ',')
head(gram.1.agg)
write.csv(gram.1.agg, 'gram1Cheat.csv', sep = ',', col.names = TRUE)
write.csv(gram.2.agg, 'gram2Cheat.csv')
write.csv(gram.3.agg, 'gram3Cheat.csv')
View(gram.1.agg)
ggplot(gram.1.agg[with(gram.1.agg, order(Count, decreasing = TRUE)), ][1:100, ], aes(x=Combo, y=Count)) + geom_bar(stat='identity') + theme(axis.text.x=element_text(angle=90, hjust=1))
Milestone Report: Exploration of Data Sets
```{r, echo = FALSE, eval = TRUE, fig.width=8, fig.height=4.5, dpi=300}
